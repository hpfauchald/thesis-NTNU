\chapter{Related work}
\label{chap:related_work}
Predicting personality traits based on Multimodal Sentiment Analysis (MSA) is something new. In essence, the problem description is composed by three components: MSA, linking emotions and personality traits, and personality prediction. This chapter aims to summarize the work contributed by various researchers across these domains. Section \ref{sec:msa_related} presents the advancements done in developing MSA models for sentiment polarity detection and emotion recognition. Studies conducted to establish the relationship between emotions and personality traits are described in Section \ref{sec:emotion-personality-link}. Finally, Section \ref{sec:personality_prediction} presents the contributions in personality detection. 

\section{Multimodal Sentiment Analysis}
\label{sec:msa_related}
Extensive research has been conducted to develop MSA frameworks that predict human emotions \cite{sebe2005multimodal}. Researchers have performed emotion recognition on data for various application domains such as spoken reviews and vlogs \cite{wollmer2013youtube}, and human-machine interactions \cite{human_machine_langlet2015}. The approaches have predicted emotions in categories based on Plutchik's work (i.e., anger, anticipation, joy, trust, fear, surprise, sadness, and disgust). With the abundance of literature on this topic, scientist like Abdullah et al. \cite{DL_abdullah2021multimodal} and others \cite{sebe2005multimodal} \cite{MER_book_Sharma2021} have published reviews on the use of Machine Learning and Deep Learning techniques for MSA. Table \ref{tab:msa_related_work} summarizes the MSA models presented in this section.  \\ 

As emotions is a multimodal phenomenon, there has been a high interest in creating models that fuse two or more modalities together (bimodal and trimodal systems.) The authors in \cite{tensor_fusion_network_2017} investigate how different modalities change the perception of an expressed emotion (inter-modality) and how each modality interact (intra-modality). They propose a model named Tensor Fusion Network that learns inter- and intra-modality end to end. Experimental results show state-of-the-art performance for unimodal and multimodal emotion recognition and sentiment analysis. \\

The researchers in \cite{Wang2018} propose a model that utilizes language, acoustic and visual features for modeling dynamically multimodal language. For a given word, the RAVEN model first uses two separate LSTM networks to compute the visual and acoustic embedding. Then, the model infers a nonverbal shift vector as the weighted average  over the visual and acoustic embedding based on the original word embedding. Lastly, RAVEN generates a  multimodal-shifted word representation by integrating the nonverbal shift vector to the original word embedding. Results show that RAVEN achieves competitive results in both sentiment analysis and emotion recognition. Additionally, based on conducted ablation studies, results show that all components within RAVEN are necessary for achieving state-of-the-art performance. \\

The task of creating appropriate multimodal fusion schemes that extract and integrate meaningful information while preserving their mutual independence is investigated in the study conducted by Han et al. \cite{bimodal-fusion1-10.1145/3462244.3479919}. They propose a Bi-Bimodal Fusion Network (BBFN) to balance the contribution of different modality pairs properly. The network consists of two Transformer-based modules that consist of two text modality pairs. These modules are trained simultaneously and the two pairs are concatenated for final prediction. Result shows that BBFN achieves state-of-the-art performance in sentiment prediction and humor detection. This study do not cover the task to predict emotions in distinct categories.  \\

In addition to COGMEN, there exist multiple important MSA frameworks for conversations in the literature. Wang et al. \cite{Af-CAN_2021} address the issue of modeling single utterance prediction for emotion recognition in conversations. Because humans perceive emotions through conversational surroundings as well as individual utterances, the authors propose an attention-based fusion framework (Af-CAN) that leverages the conversational information from both target and the other speaker for utterance-level emotion detection. For each utterance, the suggested model first extract unimodal features and fuse them utilizing an attention mechanism. Second, the gated recurrent units (GRU) framework is used to extract personal and global information from the conversation. Lastly, a contextual attention mechanism is utilized to amplify the important conversational evidence for emotion recognition. The model is applied on the IEMOCAP dataset, resulting in an accuracy of 64.6\%. The main limitation with this study, is that the researchers only tested it on one dataset. Therefore it is challenging to evaluate the model's generalizability. \\

The study conducted by Majumder et al. \cite{DialogueRNN_MAJUMDER2018124} aims to establish a MSA model for multiparty conversations. Modalities for each utterance is fused using hierarchical fusion. The context between utterances is captured utilizing a recurrent neural network (RNN). More specifically, multiple GRUs model the semantic dependency among utterances in a video. \\

The contextual relationship among utterances is investigated in \cite{bc-LSTM_poria2017context}. Modalities are fused in a hierarchical manner consisting of three steps. In the first step, unimodal features are extracted without considering the contextual content. Second, context-independent  unimodal features are fed to a contextual LSTM network to get context-sensitive unimodal feature representations for each utterance. Individual LSTM networks are used for each modality. Lastly, in the third step, output from the three previous networks are fed into a single contextual LSTM for performance evaluation. Experiments performed on various LSTM architectures shows that Bi-directional LSTM (bc-LSTM) achieves best performance across MOSI, MOUD, and IEMOCAP (4-emotion classification) with unweighted accuracy scores of 80.3 \%, 68.1 \%, and 76.1 \%, respectively. \\

The study conducted in \cite{DialogueRNN_real} examines how parties in a conversation can be treated individually by adapting to the speaker of each utterance. This method, called DialogueRNN, propose an attention mechanism over all surrounding emotion representations in a dialogue. The attention provides context from relevant future and preceding utterances. Gates recurrent units are utilized to model context, party state, and emotion representations in the dialogue. Experiments performed on IEMOCAP(6-class) dataset achieves an accuracy of 61.80\%. 
%
\begin{table}[h]
\caption{MSA presented in related work.}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|p{20mm}|p{20mm}|l|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Model name}} & \multicolumn{1}{c|}{\textbf{Year}} & \multicolumn{1}{c|}{\textbf{Fusion process}} & \multicolumn{1}{c|}{\textbf{Method}} & \mulrow{}{}{\textbf{Dataset}}  & \multicolumn{1}{c|}{\textbf{Task}} & \multicolumn{1}{c|}{\textbf{Accuracy/F1-score}} \\\hline
Tension Fusion Network \cite{tensor_fusion_network_2017} & 2017 & Tensor fusion & Tensor Fusion Network & MOSI & SA & 77.1\%  \\ \hline
RAVEN \cite{Wang2018} & 2018 & Word-Level Fusion & LSTM & MOSI \newline IEMOCAP & SA & 78.0\% \\ \hline
Bi-Bimodal Fusion Network \cite{bimodal-fusion1-10.1145/3462244.3479919} & 2021 & Bi-modal fusion & LSTM \newline Transformers  & CMU-MOSI \newline CMU-MOSEI & SA & 84.3\% \\ \hline
Attention-based fusion framework \cite{Af-CAN_2021} & 2021 & Attention-Based fusion & CNN \newline GRU & IEMOCAP & ER & 64.6\% \\ \hline
Context-Aware Hierarchical Fusion \cite{DialogueRNN_MAJUMDER2018124} & 2018 & Hierarchical fusion & RNN \newline CNN & CMU-MOSI \newline IEMOCAP & ER &  76.5\% \\ \hline
bc-LSTM \cite{bc-LSTM_poria2017context} & 2017 & Hierarchical fusion & LSTM & CMU-MOSI \newline MOUD \newline IEMOCAP & ER & 76\% \\ \hline
Interactive Conversational Memory Network \cite{ICON_hazarika-etal-2018-icon} & 2018 & Early fusion & RNN \newline CNN & IEMOCAP & ER & 64.0\% \\ \hline
DialogueRNN \cite{DialogueRNN_real} & 2019 & Attention-Based fusion & GRU \newline CNN & IEMOCAP & ER & 61.8\% \\ \hline
\end{tabular}
}
\label{tab:msa_related_work}
\end{table}
%
\newline
\indent The authors in \cite{ICON_hazarika-etal-2018-icon} propose Interactive Conversational Memory Network (ICON), a novel MSA algorithm for interpersonal conversations. Here, they focus on capturing the context of self- and intra-dependencies properties of emotional dynamics. The model uses a RNN-based memory network with multi-hop attention modeling to capture the contextual information. Results show that ICON achieves 64.0\% accuracy and 63.5\% F1 score on the IEMOCAP dataset, which is statistically significant compared to baseline models. Additionally, the conducted ablation study report that concatenating three modalities result in better performance than bimodal and unimodal settings. \\

The main limitation with the various MSA models described above is how they are tested. The different dataset utilized with the models are only in a basic conversation setting. They do not concern any particular domain. In that regards, they do not exploit how these models perform in an interview setting where the interviewee is the center of interest.  

\section{Linking emotions with personality traits}
\label{sec:emotion-personality-link}
The study conducted by Zhao et al. \cite{Zhao2018} investigates how personality traits can be linked to human emotions. In their research, they wanted to recognize the big five personality traits by analysing brain waves when participants were exposed to emotional movie clips targeting seven discrete emotions. Features from the brain activity were entered to a SVM classifier for personality trait prediction. Results show that the model performs better on extraversion, agreeableness, and conscientiousness when positive emotions are presented, higher classification accuracies for neuroticism when negative emotions are elicited, and the model achieves better performance on openness when disgust is evoked. \\

Hiegler-Ribber et al. \cite{personality_emotions_link} investigated to what extent primary emotions act as a foundation for personality and higher order emotions such as spirituality. A total of 167 participants joined the study. Here, the participants answered three questionares such as The Affective Neuroscience Personality Scales (ANPS) for primary emotions, the Big Five Inventory (BFI-44) for personality traits, and the Multidimensional Inventory for Religious/Spiritual well-being. Results shows that there is a strong relation between primary emotions and personality traits as well as between personality traits and spirituality. In particular, agreeableness and extraversion are closely linked to care and play and they are closely linked to religious/spiritual well-being. \\ 

The authors in \cite{personality-emotion1-LI2022340} investigate how emotion and personality traits can be predicted simultaneously despite the lack of datasets including both aspects labeled. To solve this issue, they propose a multitask learning framework for emotion and personality detection. Information is shared between the two tasks using Softmax Gate (SoG) and a CNN is used for prediction. In addition, a Model-Agnostic Meta-Learning is used for model optimization. Experiments on three text-based datasets show that the model is able to achieve state-of-the-art performance on across the examined datasets. \\

Another study cited Zhang el al. \cite{personality-emotion2-8897617} tries to close the research gap between emotions and personality traits by using multitask learning. The model, named PersEmoN, consists of two CNN branches; one for emotions and one for behavioral traits. These two networks share their bottom feature extraction module and an adversarial-like loss function is further employed to promote representation coherence between heterogeneous dataset sources. Experiments performed on the Aff-Wild emotion dataset and ChaLearn personality dataset show that it is feasible to jointly train emotion and corresponding personality traits. 

\section{Personality prediction}
\label{sec:personality_prediction}
Jayaratne et al. \cite{personality-prediction-questions-9121971} investigated how personality traits could be predicted in a job interview setting. Data was collected from over 46,000 participants. They answered 5-7 open-ended interview questions as well as a personality questionnaire named HEXACO. Different text representation methods were applied on the textual data and used as input features in a Random Forrest regression model with personality trait scores as target. The findings showed term frequency-inverse document frequency (TF-IDF) with Latent Dirichlet Allocation (LDA) topics performs best with a correlation of r=0.39. Therefore, this study suggests it is possible to infer one's personality based on textual data. However, this study is limited using one modality in predicting personality traits.  \\ 

The authors in \cite{video-interview3-suen2020intelligent} propose an AVI platform to predict individuals' interpersonal skills and behavioral traits. Ground truth data was collected from 57 interviewees (structured interview setting) where communications skills and personality traits were manually labeled. Then, the intervieweesâ€™ extracted facial expression features were used as inputs in a CNN model, where communication skill scores and big five traits were used as the output in the neural network. The AVI model is able to predict applicants' interpersonal communication skills, openness, agreeableness, and neuroticism with significant correlation scores (0.972, 0.987, 0.978, 0.982, respectively). On the downside, the model is incapable of detecting conscientiousness and extraversion, which is two important traits related to job performance. In addition, this research is limited to using facial expression and movement as features for the model, and do not exploit the potential of textual data or audio cues. \\

Suman et al. \cite{video-interview4-SUMAN2022107715} developed an end-to-end deep multi-modal system for personality prediction. The system uses early fusion for combining features retrieved from different modalities. Facial and ambient features are extracted from the visual modality using Multi-task Cascaded Convolutional Networks (MTCNN) and ResNet. The audio features are extracted using the VGGish Convolutional Neural Networks (VGGish CNN), and the text features are extracted using n-gram Convolutional Neural Networks (CNN). The ChaLearn LAP 2017 dataset is used for evaluation, which is set of different short YouTube clips labeled with the Big-Five traits. Results show that a combination of video, text, and audio yield and average accuracy of 91.43\% which is better than bimodal and unimodal approaches. It also shows that an early fusion architecture have comparative performance to the popular late fusion method.  \\ 



% The reason for the success of CNN in the personality prediction is investigated in [20]. Their analysis showed that, the face provides major descriptive information, which is helpful in predicting personality. 



