@article{claerbout1991scrutiny,
  title={A Scrutiny of the Introduction},
  author={Claerbout, Jon F},
  journal={The Leading Edge},
  volume={10},
  number={1},
  pages={39--41},
  year={1991},
  publisher={Society of Exploration Geophysicists}
}

@article{landes1951scrutiny,
  title={A Scrutiny of the Abstract},
  author={Landes, Kenneth K},
  journal={Bulletin of the American Association of Petroleum Geologists},
  volume={35},
  number={7},
  pages={1660},
  year={1951}
}

%%%%%%%%
% Introduction
@article{personality-definition-BERGNER2020100759,
title = {What is personality? Two myths and a definition},
journal = {New Ideas in Psychology},
volume = {57},
pages = {100759},
year = {2020},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2019.100759},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X19300728},
author = {Raymond M. Bergner},
keywords = {Personality, Definition of personality, Personality concept, Trait theory, Big 5},
abstract = {This article addresses the longstanding problem that the field of personality psychology remains in need of a consensus formulation of its core subject matter, that of the nature of “personality” itself. Part 1 of the article presents some reminders about the traditional pre-empirical status of concepts in science. Part 2 introduces and calls into question two widely accepted but nonetheless questionable propositions about the nature of personality: (a) that the term refers to an underlying causal entity within a person, and (b) that the study of personality is the study of the whole person. Part 3 presents a definition of “personality”, discussion elaborating and clarifying this definition, and an explication of the ways in which it differs from previous definitions. Part 4 discusses some benefits that accrue both to having a consensus definition in general, and to acceptance of the present definition in particular.}
}

@ARTICLE{personality-prediction-questions-9121971,
  author={Jayaratne, Madhura and Jayatilleke, Buddhi},
  journal={IEEE Access}, 
  title={Predicting Personality Using Answers to Open-Ended Interview Questions}, 
  year={2020},
  volume={8},
  number={},
  pages={115345-115355},
  doi={10.1109/ACCESS.2020.3004002}}

@article{personality-and-emotion-revelle2009personality,
  title={Personality and emotion},
  author={Revelle, William and Scherer, Klaus R},
  journal={Oxford companion to emotion and the affective sciences},
  volume={1},
  pages={304--306},
  year={2009},
  publisher={Oxford University Press Oxford, UK}
}

@article{JOSHI20201316,
title = {Video Interviewing: A Review and Recommendations for Implementation in the Era of COVID-19 and Beyond},
journal = {Academic Radiology},
volume = {27},
number = {9},
pages = {1316-1322},
year = {2020},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2020.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S1076633220302993},
author = {Aparna Joshi and David A. Bloom and Amy Spencer and Kara Gaetke-Udager and Richard H. Cohan},
keywords = {Fellowship, Interview, Radiology, Residency, Videoconference},
abstract = {Due to the COVID-19 pandemic, it is very likely that many radiology residency and fellowship programs will adopt interactive videoconference interviewing for the 2020-2021 residency match cycle. Although video interviewing has become a common part of the hiring process for business, experience with video interviewing for resident and fellow selection has been limited. Advantages of video interviews over traditional on-site interviews include cost-savings to both applicants and residency programs, less disruption to an applicant's educational activities, and potential for training programs to access a wider pool of candidates. The loss of the casual interactions that occur during an on-site interview and the inability of candidates to evaluate training facilities and their surrounding environments in-person are among the obstacles posed by video interviews, but training programs can mitigate these challenges with enhanced website content and creative media solutions. Through a review of the existing literature and internet resources, this article recommends specific measures medical schools, applicants, and radiology residency and fellowship programs can take to optimize the virtual interview experience for all involved parties.}
}


@book{SA-definition,
author = {Liu, B.},
title = {Sentiment analysis: Mining opinions, sentiments, and emotions},
year = {2015},
month = {January},
pages = {1-367},
doi = {10.1017/CBO9781139084789}
}

@Article{HP-integration-project,
  author  = "Andreas Kilde Lien and Lars Martin Randem and Hans Petter Fauchald Taralrud",
  title   = "OSN Analysis Dashboard",
  journal = "Integration Project",
  year    = "2022"
}

@inproceedings{describe-polarity-dave2003mining,
  title={Mining the peanut gallery: Opinion extraction and semantic classification of product reviews},
  author={Dave, Kushal and Lawrence, Steve and Pennock, David M},
  booktitle={Proceedings of the 12th international conference on World Wide Web},
  pages={519--528},
  year={2003}
}

@article{SA-history-MANTYLA201816,
title = {The evolution of sentiment analysis—A review of research topics, venues, and top cited papers},
journal = {Computer Science Review},
volume = {27},
pages = {16-32},
year = {2018},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574013717300606},
author = {Mika V. Mäntylä and Daniel Graziotin and Miikka Kuutila},
keywords = {Sentiment analysis, Opinion mining, Bibliometric study, Text mining, Literature review, Topic modeling, Latent Dirichlet Allocation, Qualitative analysis},
abstract = {Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging to keep track of all the activities in the area. We present a computer-assisted literature review, where we utilize both text mining and qualitative coding, and analyze 6996 papers from Scopus. We find that the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th century and in the text subjectivity analysis performed by the computational linguistics community in 1990’s. However, the outbreak of computer-based sentiment analysis only occurred with the availability of subjective texts on the Web. Consequently, 99\% of the papers have been published after 2004. Sentiment analysis papers are scattered to multiple publication venues, and the combined number of papers in the top-15 venues only represent ca. 30\% of the papers in total. We present the top-20 cited papers from Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering and cyberbullying extend the utilization of sentiment analysis.}
}

@Article{Student-feedback-MOOCS-app11093986,
AUTHOR = {Kastrati, Zenun and Dalipi, Fisnik and Imran, Ali Shariq and Pireva Nuci, Krenare and Wani, Mudasir Ahmad},
TITLE = {Sentiment Analysis of Students’ Feedback with NLP and Deep Learning: A Systematic Mapping Study},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3986},
URL = {https://www.mdpi.com/2076-3417/11/9/3986},
ISSN = {2076-3417},
ABSTRACT = {In the last decade, sentiment analysis has been widely applied in many domains, including business, social networks and education. Particularly in the education domain, where dealing with and processing students’ opinions is a complicated task due to the nature of the language used by students and the large volume of information, the application of sentiment analysis is growing yet remains challenging. Several literature reviews reveal the state of the application of sentiment analysis in this domain from different perspectives and contexts. However, the body of literature is lacking a review that systematically classifies the research and results of the application of natural language processing (NLP), deep learning (DL), and machine learning (ML) solutions for sentiment analysis in the education domain. In this article, we present the results of a systematic mapping study to structure the published information available. We used a stepwise PRISMA framework to guide the search process and searched for studies conducted between 2015 and 2020 in the electronic research databases of the scientific literature. We identified 92 relevant studies out of 612 that were initially found on the sentiment analysis of students’ feedback in learning platform environments. The mapping results showed that, despite the identified challenges, the field is rapidly growing, especially regarding the application of DL, which is the most recent trend. We identified various aspects that need to be considered in order to contribute to the maturity of research and development in the field. Among these aspects, we highlighted the need of having structured datasets, standardized solutions and increased focus on emotional expression and detection.},
DOI = {10.3390/app11093986}
}

@article{mining-methods-droba1931methods,
  title={Methods used for measuring public opinion},
  author={Droba, DD},
  journal={American Journal of Sociology},
  volume={37},
  number={3},
  pages={410--423},
  year={1931},
  publisher={University of Chicago Press}
}

@book{ATHEN-voting-thorley2012athenian,
  title={Athenian democracy},
  author={Thorley, John},
  year={2012},
  publisher={Routledge}
}

@article{deep-learning-WIRE,
author = {Zhang, Lei and Wang, Shuai and Liu, Bing},
title = {Deep learning for sentiment analysis: A survey},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {8},
number = {4},
pages = {e1253},
keywords = {data mining, deep learning, machine learning, natural language processing, neural network, opinion mining, sentiment analysis, survey},
doi = {https://doi.org/10.1002/widm.1253},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1253},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1253},
abstract = {Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis. This article is categorized under: Fundamental Concepts of Data and Knowledge > Data Concepts Algorithmic Development > Text Mining},
year = {2018}
}

@article{marketing-sa-rambocas2018online,
  title={Online sentiment analysis in marketing research: a review},
  author={Rambocas, Meena and Pacheco, Barney G},
  journal={Journal of Research in Interactive Marketing},
  year={2018},
  publisher={Emerald Publishing Limited}
}

@INPROCEEDINGS{stock_price1,  
    author={Kim, Jaeyoon and Seo, Jangwon and Lee, Minhyeok and Seok, Junhee},  
    booktitle={2019 Eleventh International Conference on Ubiquitous and Future Networks (ICUFN)},   
    title={Stock Price Prediction Through the Sentimental Analysis of News Articles},   
    year={2019},  
    volume={},  
    number={},  
    pages={700-702}, 
    doi={10.1109/ICUFN.2019.8806182}
}

@article{political-science-sa-rice2021corpus,
  title={Corpus-based dictionaries for sentiment analysis of specialized vocabularies},
  author={Rice, Douglas R and Zorn, Christopher},
  journal={Political Science Research and Methods},
  volume={9},
  number={1},
  pages={20--35},
  year={2021},
  publisher={Cambridge University Press}
}

@ARTICLE{education_domain,  
    author={Kastrati, Zenun and Imran, Ali Shariq and Kurti, Arianit},  
    journal={IEEE Access},   title={Weakly Supervised Framework for Aspect-Based Sentiment Analysis on Students’ Reviews of MOOCs},   
    year={2020},  
    volume={8},  
    number={},  
    pages={106799-106810},  
    doi={10.1109/ACCESS.2020.3000739}
}

@article{healthcare-sa-gohil2018sentiment,
  title={Sentiment analysis of health care tweets: review of the methods used},
  author={Gohil, Sunir and Vuik, Sabine and Darzi, Ara and others},
  journal={JMIR public health and surveillance},
  volume={4},
  number={2},
  pages={e5789},
  year={2018},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@article{MSA_review1_SOLEYMANI20173,
title = {A survey of multimodal sentiment analysis},
journal = {Image and Vision Computing},
volume = {65},
pages = {3-14},
year = {2017},
note = {Multimodal Sentiment Analysis and Mining in the Wild Image and Vision Computing},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0262885617301191},
author = {Mohammad Soleymani and David Garcia and Brendan Jou and Björn Schuller and Shih-Fu Chang and Maja Pantic},
keywords = {Sentiment, Affect, Sentiment analysis, Human behavior analysis, Computer vision, Affective computing},
abstract = {Sentiment analysis aims to automatically uncover the underlying attitude that we hold towards an entity. The aggregation of these sentiment over a population represents opinion polling and has numerous applications. Current text-based sentiment analysis rely on the construction of dictionaries and machine learning models that learn sentiment from large text corpora. Sentiment analysis from text is currently widely used for customer satisfaction assessment and brand perception analysis, among others. With the proliferation of social media, multimodal sentiment analysis is set to bring new opportunities with the arrival of complementary data streams for improving and going beyond text-based sentiment analysis. Since sentiment can be detected through affective traces it leaves, such as facial and vocal displays, multimodal sentiment analysis offers promising avenues for analyzing facial and vocal expressions in addition to the transcript or textual content. These approaches leverage emotion recognition and context inference to determine the underlying polarity and scope of an individual's sentiment. In this survey, we define sentiment and the problem of multimodal sentiment analysis and review recent developments in multimodal sentiment analysis in different domains, including spoken reviews, images, video blogs, human–machine and human–human interactions. Challenges and opportunities of this emerging field are also discussed leading to our thesis that multimodal sentiment analysis holds a significant untapped potential.}
}

%%%%%%%%
% Background
@article{hiring-process-Sołek-BorowskaWilczewska+2018+25+33,
author = {Celina Sołek-Borowska and Maja Wilczewska},
doi = {doi:10.2478/jec-2018-0017},
url = {https://doi.org/10.2478/jec-2018-0017},
title = {New Technologies in the Recruitment Process},
journal = {Economics and Culture},
number = {2},
volume = {15},
year = {2018},
pages = {25--33}
}

@article{robotic-process-nawaz2019robotic,
  title={Robotic process automation for recruitment process},
  author={Nawaz, Dr Nishad},
  journal={International Journal of Advanced Research in Engineering and Technology},
  volume={10},
  number={2},
  year={2019}
}

@article{video-interview1-LUKACIK2022100789,
title = {Into the void: A conceptual model and research agenda for the design and use of asynchronous video interviews},
journal = {Human Resource Management Review},
volume = {32},
number = {1},
pages = {100789},
year = {2022},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2020.100789},
url = {https://www.sciencedirect.com/science/article/pii/S1053482220300620},
author = {Eden-Raye Lukacik and Joshua S. Bourdage and Nicolas Roulin},
keywords = {Asynchronous video interview, Digital interview, Interview design, Technology-mediated interview, Applicant reactions},
abstract = {Asynchronous video interviews (AVIs) are a form of one-way, technology-mediated, selection interviewing that continue to grow in popularity. An AVI is a broad method that varies substantially in design and execution. Despite being adopted by many organizations, human resources professionals, and hiring managers, research on AVIs is lagging far behind practice. Empirical evidence is scarce and conceptual work to guide research efforts and best practice recommendations is lacking. We propose a framework for examining the role and impact of specific design features of AVIs, building on theories of justice-based applicant reactions, social presence, interview anxiety, and impression management. More precisely, our framework highlights how pre-interview design decisions by organizations and completion decisions by applicants can influence reactions and behaviors during the interview, as well as post-interview outcomes. As such, we offer an agenda of the central topics that need to be addressed, and a set of testable propositions to guide future research.}
}

@article{video_interview2-brenner2016asynchronous,
  title={Asynchronous video interviewing as a new technology in personnel selection: The applicant’s point of view},
  author={Brenner, Falko S and Ortner, Tuulia M and Fay, Doris},
  journal={Frontiers in psychology},
  volume={7},
  pages={863},
  year={2016},
  publisher={Frontiers Media SA}
}

@inproceedings{COGMEN_joshi-etal-2022-cogmen,
    title = "{COGMEN}: {CO}ntextualized {GNN} based Multimodal Emotion recognitio{N}",
    author = "Joshi, Abhinav  and
      Bhat, Ashwani  and
      Jain, Ayush  and
      Singh, Atin  and
      Modi, Ashutosh",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.306",
    doi = "10.18653/v1/2022.naacl-main.306",
    pages = "4148--4164",
    abstract = "Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person{'}s emotions are influenced by the other speaker{'}s utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi- modal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the- art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.",
}

@INPROCEEDINGS{MSA-review-3-9686504,
  author={Gandhi, Ankita and Adhvaryu, Kinjal and Khanduja, Vidhi},
  booktitle={2021 IEEE Pune Section International Conference (PuneCon)}, 
  title={Multimodal Sentiment Analysis: Review, Application Domains and Future Directions}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/PuneCon52575.2021.9686504}}



%%%%%%%%
% Related work
@article{video-interview3-suen2020intelligent,
  title={Intelligent video interview agent used to predict communication skill and perceived personality traits},
  author={Suen, Hung-Yue and Hung, Kuo-En and Lin, Chien-Liang},
  journal={Human-centric Computing and Information Sciences},
  volume={10},
  pages={1--12},
  year={2020},
  publisher={Springer}
}

@article{video-interview4-SUMAN2022107715,
title = {A multi-modal personality prediction system},
journal = {Knowledge-Based Systems},
volume = {236},
pages = {107715},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107715},
url = {https://www.sciencedirect.com/science/article/pii/S095070512100962X},
author = {Chanchal Suman and Sriparna Saha and Aditya Gupta and Saurabh Kumar Pandey and Pushpak Bhattacharyya},
keywords = {Personality prediction, Multi-modal, CNN, ResNet},
abstract = {The behavior, mental-health, emotion, life choices, social nature, and thought patterns of an individual are revealed by personality. Cyber forensics, personalized services, recommender systems are some of the examples of automatic personality prediction. A deep learning based personality prediction system has been developed in this work. Facial and ambient features are extracted from the visual modality using Multi-task Cascaded Convolutional Networks (MTCNN) and ResNet, respectively; the audio features are extracted using the VGGish Convolutional Neural Networks (VGGish CNN), and the text features are extracted using n-gram Convolutional Neural Networks (CNN). The extracted features are then passed to a fully connected layer followed by sigmoid for the final output prediction. Finally, the text, visual and audio modalities are combined in different ways: (i) concatenation of features in multi-modal setting, and (ii) application of different attention mechanisms for fusing features. The dataset released in Chalearn-17 is used for evaluating the performance of the system. From the obtained results, it can be concluded that, the concatenation of features extracted from different modalities attains comparable results with the averaging method (late fusion). It is also shown that a hand full of images are enough for attaining comparable performance.}
}

@article{personality-emotion1-LI2022340,
title = {Multitask learning for emotion and personality traits detection},
journal = {Neurocomputing},
volume = {493},
pages = {340-350},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.049},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222004180},
author = {Yang Li and Amirmohammad Kazemeini and Yash Mehta and Erik Cambria},
keywords = {Personality traits detection, Emotion detection, Multitask learning, Information sharing gate},
abstract = {In recent years, deep learning-based automated personality traits detection has received a lot of attention, especially now, due to the massive digital footprints of an individual. Moreover, many researchers have demonstrated that there is a strong link between personality traits and emotions. In this paper, we build on the known correlation between personality traits and emotional behaviors and propose a novel transferring based multitask learning framework that simultaneously predicts both of them. We also empirically evaluate and discuss different information-sharing mechanisms between the two tasks. To ensure the high quality of the learning process, we adopt a model-agnostic meta-learning-like framework for model optimization. Our computationally efficient multitask learning model achieves the state-of-the-art performance across multiple famous personality and emotion datasets, even outperforming language model-based models.}
}

@ARTICLE{personality-emotion2-8897617,
  author={Zhang, Le and Peng, Songyou and Winkler, Stefan},
  journal={IEEE Transactions on Affective Computing}, 
  title={PersEmoN: A Deep Network for Joint Analysis of Apparent Personality, Emotion and Their Relationship}, 
  year={2022},
  volume={13},
  number={1},
  pages={298-305},
  doi={10.1109/TAFFC.2019.2951656}}

@inproceedings{bimodal-fusion1-10.1145/3462244.3479919,
author = {Han, Wei and Chen, Hui and Gelbukh, Alexander and Zadeh, Amir and Morency, Louis-philippe and Poria, Soujanya},
title = {Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479919},
doi = {10.1145/3462244.3479919},
abstract = {Multimodal sentiment analysis aims to extract and integrate semantic information collected from multiple modalities to recognize the expressed emotions and sentiment in multimodal data. This research area’s major concern lies in developing an extraordinary fusion scheme that can extract and integrate key information from various modalities. However, previous work is restricted by the lack of leveraging dynamics of independence and correlation between modalities to reach top performance. To mitigate this, we propose the Bi-Bimodal Fusion Network (BBFN), a novel end-to-end network that performs fusion (relevance increment) and separation (difference increment) on pairwise modality representations. The two parts are trained simultaneously such that the combat between them is simulated. The model takes two bimodal pairs as input due to the known information imbalance among modalities. In addition, we leverage a gated control mechanism in the Transformer architecture to further improve the final output. Experimental results on three datasets (CMU-MOSI, CMU-MOSEI, and UR-FUNNY) verifies that our model significantly outperforms the SOTA. The implementation of this work is available at https://github.com/declare-lab/multimodal-deep-learning and https://github.com/declare-lab/BBFN.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {6–15},
numpages = {10},
keywords = {multimodal representations, multimodal fusion, cross-modal processing},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%
%Introduction
@misc{conversations_a_day,
  author       = "Marcel Schwantes",
  title        = "The 1 Question the Most Interesting People Will Ask to Start Great Conversations",
  howpublished = "\url{https://www.inc.com/marcel-schwantes/the-1-question-most-interesting-people-will-ask-to-start-great-conversations.html}",
  month        = "February",
  year         = "2020",
  note         = "[online] accessed 01.10.2022"
}


@article{conversation_easy_GARROD20048,
title = {Why is conversation so easy?},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {1},
pages = {8-11},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2003.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S136466130300295X},
author = {Simon Garrod and Martin J. Pickering},
abstract = {Traditional accounts of language processing suggest that monologue – presenting and listening to speeches – should be more straightforward than dialogue – holding a conversation. This is clearly not the case. We argue that conversation is easy because of an interactive processing mechanism that leads to the alignment of linguistic representations between partners. Interactive alignment occurs via automatic alignment channels that are functionally similar to the automatic links between perception and behaviour (the so-called perception–behaviour expressway) proposed in recent accounts of social interaction. We conclude that humans are ‘designed’ for dialogue rather than monologue.}
}


@inproceedings{COGMEN_joshi-etal-2022-cogmen,
    title = "{COGMEN}: {CO}ntextualized {GNN} based Multimodal Emotion recognitio{N}",
    author = "Joshi, Abhinav  and
      Bhat, Ashwani  and
      Jain, Ayush  and
      Singh, Atin  and
      Modi, Ashutosh",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.306",
    doi = "10.18653/v1/2022.naacl-main.306",
    pages = "4148--4164",
    abstract = "Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person{'}s emotions are influenced by the other speaker{'}s utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi- modal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the- art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.",
}

@Article{HP_RPP,
  author  = "Hans Petter Fauchald Taralrud",
  title   = "Multimodal Sentiment Analysis",
  journal = "Research Project Planning",
  year    = "2022"
}

@ARTICLE{emotion_personalty_correlation_Zhao2018,
  author={Zhao, Guozhen and Ge, Yan and Shen, Biying and Wei, Xingjie and Wang, Hao},
  journal={IEEE Transactions on Affective Computing}, 
  title={Emotion Analysis for Personality Inference from EEG Signals}, 
  year={2018},
  volume={9},
  number={3},
  pages={362-371},
  doi={10.1109/TAFFC.2017.2786207}}

@misc{personality_for_emploment,
  author       = "Alexandra Jerselius and Nathaniel Pollard",
  title        = "Hyper Island Executive Study: Personality Trumps Skill in Search for Digital Talent",
  howpublished = "\url{https://www.trustcollective.com/2014/03/24/hyper-island-executive-study-personality-trumps-skill-in-search-for-digital-talent/}",
  month        = "",
  year         = "2014",
  note         = "[online] accessed 25.10.2022"
}

@article{sentiment_emotion_difference_munezero2014they,
  title={Are they different? Affect, feeling, emotion, sentiment, and opinion detection in text},
  author={Munezero, Myriam and Montero, Calkin Suero and Sutinen, Erkki and Pajunen, John},
  journal={IEEE transactions on affective computing},
  volume={5},
  number={2},
  pages={101--111},
  year={2014},
  publisher={IEEE}
}

@article{scherer2005emotions,
  title={What are emotions? And how can they be measured?},
  author={Scherer, Klaus R},
  journal={Social science information},
  volume={44},
  number={4},
  pages={695--729},
  year={2005},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@book{deonna2012emotions,
  title={The emotions: A philosophical introduction},
  author={Deonna, Julien and Teroni, Fabrice},
  year={2012},
  publisher={Routledge}
}

@ARTICLE{personality_emotions_link,
AUTHOR={Hiebler-Ragger, Michaela and Fuchshuber, Jürgen and Dröscher, Heidrun and Vajda, Christian and Fink, Andreas and Unterrainer, Human F.}, 
TITLE={Personality Influences the Relationship Between Primary Emotions and Religious/Spiritual Well-Being},      
JOURNAL={Frontiers in Psychology},      
VOLUME={9},           
YEAR={2018},      
URL={https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00370},       
DOI={10.3389/fpsyg.2018.00370},      
ISSN={1664-1078},   
ABSTRACT={The study of human emotions and personality provides valuable insights into the parameters of mental health and well-being. Affective neuroscience proposes that several levels of emotions – ranging from primary ones such as LUST or FEAR up to higher emotions such as spirituality – interact on a neural level. The present study aimed to further explore this theory. Furthermore, we hypothesized that personality – formed by bottom-up primary emotions and cortical top-down regulation – might act as a link between primary emotions and religious/spiritual well-being. A total sample of 167 (78\% female) student participants completed the Affective Neuroscience Personality Scale (primary emotions), the Big Five Personality Inventory and the Multidimensional Inventory of Religious/Spiritual Well-Being (higher emotions). Correlation analyses confirmed the link between primary and higher emotions as well as their relation to personality. Further regression analyses indicated that personality dimensions mediate the relationship between primary and higher emotions. A substantial interaction between primary emotions, personality dimensions, and religious/spiritual well-being could be confirmed. From a developmental perspective, cortical top-down regulation might influence religious/spiritual well-being by forming relevant personality dimensions. Hence, CARE as well as Agreeableness seem of special importance. Future studies might focus on implications for clinical groups.}
}

@ARTICLE{cross_cultural,
  author={Imran, Ali Shariq and Daudpota, Sher Muhammad and Kastrati, Zenun and Batra, Rakhi},
  journal={IEEE Access}, 
  title={Cross-Cultural Polarity and Emotion Detection Using Sentiment Analysis and Deep Learning on COVID-19 Related Tweets}, 
  year={2020},
  volume={8},
  number={},
  pages={181074-181090},
  doi={10.1109/ACCESS.2020.3027350}}

@Book{panksepp_book,
  author    = "J. Panksepp",
  title     = "Affective Neuroscience: The Foundations of Human and Animal Emotions",
  publisher = "NY: Oxford University Press",
  year      = "1998"
}

@incollection{plutchik_model,
title = {Chapter 1 - A GENERAL PSYCHOEVOLUTIONARY THEORY OF EMOTION},
editor = {Robert Plutchik and Henry Kellerman},
booktitle = {Theories of Emotion},
publisher = {Academic Press},
pages = {3-33},
year = {1980},
isbn = {978-0-12-558701-3},
doi = {https://doi.org/10.1016/B978-0-12-558701-3.50007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780125587013500077},
author = {ROBERT PLUTCHIK},
abstract = {ABSTRACT
The general psychoevolutionary theory of emotion that is presented here has a number of important characteristics. First, it provides a broad evolutionary foundation for conceptualizing the domain of emotion as seen in animals and humans. Second, it provides a structural model which describes the interrelations among emotions. Third, it has demonstrated both theoretical and empirical relations among a number of derivative domains including personality traits, diagnoses, and ego defenses. Fourth, it has provided a theoretical rationale for the construction of tests and scales for the measurement of key dimensions within these various domains. Fifth, it has stimulated a good deal of empirical research using these tools and concepts. Finally, the theory provides useful insights into the relationships among emotions, adaptations, and evolution.}
}

%%%%%%%%
%Related work
@inproceedings{sebe2005multimodal,
  title={Multimodal approaches for emotion recognition: a survey},
  author={Sebe, Nicu and Cohen, Ira and Gevers, Theo and Huang, Thomas S},
  booktitle={Internet Imaging VI},
  volume={5670},
  pages={56--67},
  year={2005},
  organization={SPIE}
}

@article{wollmer2013youtube,
  title={Youtube movie reviews: Sentiment analysis in an audio-visual context},
  author={W{\"o}llmer, Martin and Weninger, Felix and Knaup, Tobias and Schuller, Bj{\"o}rn and Sun, Congkai and Sagae, Kenji and Morency, Louis-Philippe},
  journal={IEEE Intelligent Systems},
  volume={28},
  number={3},
  pages={46--53},
  year={2013},
  publisher={IEEE}
}

@inproceedings{human_machine_langlet2015,
  title={Adapting sentiment analysis to face-to-face human-agent interactions: from the detection to the evaluation issues},
  author={Langlet, Caroline and Clavel, Chlo{\'e}},
  booktitle={2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},
  pages={14--20},
  year={2015},
  organization={IEEE}
}

@article{DL_abdullah2021multimodal,
  title={Multimodal emotion recognition using deep learning},
  author={Abdullah, Sharmeen M Saleem Abdullah and Ameen, Siddeeq Y Ameen and Sadeeq, Mohammed AM and Zeebaree, Subhi},
  journal={Journal of Applied Science and Technology Trends},
  volume={2},
  number={02},
  pages={52--58},
  year={2021}
}

@Inbook{MER_book_Sharma2021,
author="Sharma, Garima
and Dhall, Abhinav",
editor="Phillips-Wren, Gloria
and Esposito, Anna
and Jain, Lakhmi C.",
title="A Survey on Automatic Multimodal Emotion Recognition in the Wild",
bookTitle="Advances in Data Science: Methodologies and Applications",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="35--64",
abstract="Affective computing has been an active area of research for the past two decades. One of the major component of affective computing is automatic emotion recognition. This chapter gives a detailed overview of different emotion recognition techniques and the predominantly used signal modalities. The discussion starts with the different emotion representations and their limitations. Given that affective computing is a data-driven research area, a thorough comparison of standard emotion labelled databases is presented. Based on the source of the data, feature extraction and analysis techniques are presented for emotion recognition. Further, applications of automatic emotion recognition are discussed along with current and important issues such as privacy and fairness.",
isbn="978-3-030-51870-7",
doi="10.1007/978-3-030-51870-7_3",
url="https://doi.org/10.1007/978-3-030-51870-7_3"
}

@inproceedings{tensor_fusion_network_2017,
    title = "Tensor Fusion Network for Multimodal Sentiment Analysis",
    author = "Zadeh, Amir  and
      Chen, Minghai  and
      Poria, Soujanya  and
      Cambria, Erik  and
      Morency, Louis-Philippe",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1115",
    doi = "10.18653/v1/D17-1115",
    pages = "1103--1114",
    abstract = "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Networks, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.",
}

@INPROCEEDINGS{Af-CAN_2021,  author={Wang, Tana and Hou, Yaqing and Zhou, Dongsheng and Zhang, Qiang},  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},   title={A Contextual Attention Network for Multimodal Emotion Recognition in Conversation},   year={2021},  volume={},  number={},  pages={1-7},  doi={10.1109/IJCNN52387.2021.9533718}}

@article{DialogueRNN_MAJUMDER2018124,
title = {Multimodal sentiment analysis using hierarchical fusion with context modeling},
journal = {Knowledge-Based Systems},
volume = {161},
pages = {124-133},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.07.041},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118303897},
author = {N. Majumder and D. Hazarika and A. Gelbukh and E. Cambria and S. Poria},
keywords = {Multimodal fusion, Sentiment analysis},
abstract = {Multimodal sentiment analysis is a very actively growing field of research. A promising area of opportunity in this field is to improve the multimodal fusion mechanism. We present a novel feature fusion strategy that proceeds in a hierarchical fashion, first fusing the modalities two in two and only then fusing all three modalities. On multimodal sentiment analysis of individual utterances, our strategy outperforms conventional concatenation of features by 1\%, which amounts to 5\% reduction in error rate. On utterance-level multimodal sentiment analysis of multi-utterance video clips, for which current state-of-the-art techniques incorporate contextual information from other utterances of the same clip, our hierarchical fusion gives up to 2.4\% (almost 10\% error rate reduction) over currently used concatenation. The implementation of our method is publicly available in the form of open-source code.}
}

@inproceedings{ICON_hazarika-etal-2018-icon,
    title = "{ICON}: Interactive Conversational Memory Network for Multimodal Emotion Detection",
    author = "Hazarika, Devamanyu  and
      Poria, Soujanya  and
      Mihalcea, Rada  and
      Cambria, Erik  and
      Zimmermann, Roger",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1280",
    doi = "10.18653/v1/D18-1280",
    pages = "2594--2604",
    abstract = "Emotion recognition in conversations is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our model outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.",
}

%%%%%%%%%
% Preliminaries
@misc{meld_dataset,
  doi = {10.48550/ARXIV.1810.02508},
  url = {https://arxiv.org/abs/1810.02508},
  author = {Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations},
  publisher = {arXiv},
  year = {2018},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@inproceedings{cmu-mosei_zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}

@article{MSA_review2_GANDHI2023424,
title = {Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions},
journal = {Information Fusion},
volume = {91},
pages = {424-444},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
author = {Ankita Gandhi and Kinjal Adhvaryu and Soujanya Poria and Erik Cambria and Amir Hussain},
keywords = {Affective computing, Sentiment analysis, Multimodal fusion, Fusion techniques},
abstract = {Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for multimodal feature extraction and fusion and sentiment polarity detection, with aims to minimize error rate and improve performance. This survey paper examines primary taxonomy and newly released multimodal fusion architectures. Recent developments in MSA architectures are divided into ten categories, namely early fusion, late fusion, hybrid fusion, model-level fusion, tensor fusion, hierarchical fusion, bi-modal fusion, attention-based fusion, quantum-based fusion and word-level fusion. A comparison of several architectural evolutions in terms of MSA fusion categories and their relative strengths and limitations are presented. Finally, a number of interdisciplinary applications and future research directions are proposed.}
}

@Article{iemocap_dataset,
  author  = "Carlos Busso and Murtaza Bulut and Chi-Chun Lee and Abe Kazemzadeh and Emily Mower and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan",
  title   = "IEMOCAP: interactive emotional dyadic motion capture database",
  journal = "Language Resources and Evaluation",
  year    = "2008",
  volume  = "42",
  pages   = "335--359",
  month   = "November"
}

%%%%%%%
% Experiment
@misc{cogmen_implementation,
  author       = "",
  title        = "COGMEN",
  howpublished = "\url{https://github.com/exploration-lab/cogmen}",
  year         = "2022",
  note         = "[online] accessed 09.09.2022"
}

@INPROCEEDINGS{openface,  author={Baltrusaitis, Tadas and Zadeh, Amir and Lim, Yao Chong and Morency, Louis-Philippe},  booktitle={2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)},   title={OpenFace 2.0: Facial Behavior Analysis Toolkit},   year={2018},  volume={},  number={},  pages={59-66},  doi={10.1109/FG.2018.00019}}

@inproceedings{opensmile,
author = {Eyben, Florian and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn},
title = {Opensmile: The Munich Versatile and Fast Open-Source Audio Feature Extractor},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874246},
doi = {10.1145/1873951.1874246},
abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1459–1462},
numpages = {4},
keywords = {statistical functionals, audio feature extraction, signal processing, speech, emotion, music},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{sBERT,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@inproceedings{librosa_mcfee2015librosa,
  title={librosa: Audio and music signal analysis in python},
  author={McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
  booktitle={Proceedings of the 14th python in science conference},
  volume={8},
  pages={18--25},
  year={2015}
}

%%%%%%%
% Discussion
@inproceedings{bc-LSTM_poria2017context,
  title={Context-dependent sentiment analysis in user-generated videos},
  author={Poria, Soujanya and Cambria, Erik and Hazarika, Devamanyu and Majumder, Navonil and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: Long papers)},
  pages={873--883},
  year={2017}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%
%Introduction
@article{SOLEYMANI20173,
title = {A survey of multimodal sentiment analysis},
journal = {Image and Vision Computing},
volume = {65},
pages = {3-14},
year = {2017},
note = {Multimodal Sentiment Analysis and Mining in the Wild Image and Vision Computing},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0262885617301191},
author = {Mohammad Soleymani and David Garcia and Brendan Jou and Björn Schuller and Shih-Fu Chang and Maja Pantic},
keywords = {Sentiment, Affect, Sentiment analysis, Human behavior analysis, Computer vision, Affective computing}
}

@article{GANDHI2023424,
title = {Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions},
journal = {Information Fusion},
volume = {91},
pages = {424-444},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
author = {Ankita Gandhi and Kinjal Adhvaryu and Soujanya Poria and Erik Cambria and Amir Hussain},
keywords = {Affective computing, Sentiment analysis, Multimodal fusion, Fusion techniques}
}

@INPROCEEDINGS{stock_price1,  
    author={Kim, Jaeyoon and Seo, Jangwon and Lee, Minhyeok and Seok, Junhee},  
    booktitle={2019 Eleventh International Conference on Ubiquitous and Future Networks (ICUFN)},   
    title={Stock Price Prediction Through the Sentimental Analysis of News Articles},   
    year={2019},  
    volume={},  
    number={},  
    pages={700-702}, 
    doi={10.1109/ICUFN.2019.8806182}
}

@ARTICLE{education_domain,  
    author={Kastrati, Zenun and Imran, Ali Shariq and Kurti, Arianit},  
    journal={IEEE Access},   title={Weakly Supervised Framework for Aspect-Based Sentiment Analysis on Students’ Reviews of MOOCs},   
    year={2020},  
    volume={8},  
    number={},  
    pages={106799-106810},  
    doi={10.1109/ACCESS.2020.3000739}
}

@INPROCEEDINGS{politics_prediction,  
    author={Nausheen, Farha and Begum, Sayyada Hajera},  
    booktitle={2018 2nd International Conference on Inventive Systems and Control (ICISC)},   
    title={Sentiment analysis to predict election results using Python},   
    year={2018},  
    volume={},  
    number={},  
    pages={1259-1262},  
    doi={10.1109/ICISC.2018.8399007}
}

%%%%%%%%%
%Background
@article{ATS2015,
  author  = "Sven Laumer, Christian Maier \& Andreas Eckhardt",
  title   = "The impact of business process management and applicant tracking systems on recruiting process performance: an empirical study",
  journal = "Journal of Business Economics",
  year    = 2015,
  volume  = "",
  number  = "85",
  pages   = "421--453"
}

@INPROCEEDINGS{Chen2017_video_interview,
  author={Chen, Lei and Zhao, Ru and Leong, Chee Wee and Lehman, Blair and Feng, Gary and Hoque, Mohammed Ehsan},
  booktitle={2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)}, 
  title={Automated video interview judgment on a large-sized corpus collected online}, 
  year={2017},
  volume={},
  number={},
  pages={504-509},
  doi={10.1109/ACII.2017.8273646}}
  
@article{ONEILL2013162,
title = {The impact of “non-targeted traits” on personality test faking, hiring, and workplace deviance},
journal = {Personality and Individual Differences},
volume = {55},
number = {2},
pages = {162-168},
year = {2013},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2013.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0191886913001104},
author = {Thomas A. O’Neill and Naomi M. Lee and Jelena Radan and Stephanie J. Law and Rhys J. Lewis and Julie J. Carswell},
keywords = {pre-employment personality tests, personality, faking, faking warning, Big Five},
abstract = {Evidence suggests that job applicants often “fake” on pre-employment personality tests by attempting to portray an exceedingly desirable impression in order to improve the likelihood of being selected. In the current research we shed light on the personality characteristics of those individuals who seem most likely to engage in faking. We refer to these personality variables as non-targeted traits when they are not directly targeted by the organization’s pre-employment personality test. These traits, however, may have an influence on targeted scores used for employment decision making through their effect on faking. Findings suggest that individuals will be more likely to be hired if they are low on non-targeted traits including Honesty–Humility, Integrity, and Morality, and high on Risk Taking. Such individuals also reported higher levels of workplace deviance in their current jobs. Thus, it seems that individuals low on Honesty–Humility, Integrity, and Morality, and individuals high on Risk Taking, may be most likely to engage in personality test faking, be hired, and participate in workplace deviant behaviors if these traits are not directly targeted in selection.}
}

@article{personality_goldberg_1990,
  author  = "Lewis R. Goldberg",
  title   = "An Alternative Description of Personality: The Big-Five Factor Structure ",
  journal = "Journal of Personality and Social Psychology",
  year    = "1990",
  volume  = "59",
  number  = "6",
  pages   = "1216--1229"
}

@Book{best_trait,
  author    = "Stephen P. Robbins and Timothy A. Judge",
  title     = "Essentials of Organizational Behavior",
  publisher = "Pearson",
  year      = "2018",
  edition   = "12th"
}

@article{big5-image,
author = {Gungea, Meera and Jaunky, Vishal and Ramesh, Vani},
year = {2017},
month = {03},
pages = {42-46},
title = {Personality Traits and Juvenile Delinquency: A critical analysis},
volume = {5},
journal = {International Journal of Conceptions on Management and Social Sciences}
}

@misc{important_personality,
  author       = "Alexandra Jerselius and Nathaniel Pollard",
  title        = "Hyper Island Executive Study: Personality Trumps Skill in Search for Digital Talent",
  howpublished = "\url{https://www.trustcollective.com/2014/03/24/hyper-island-executive-study-personality-trumps-skill-in-search-for-digital-talent/}",
  month        = "",
  year         = "2014",
  note         = "[online] accessed 25.10.2022",
  annote       = ""
}

Fiske, D. W. (1949). Consistency of the factorial structures of personality ratings from different sources. Journal of Abnormal and Social
Psychology, 44, 329-344. 

@ARTICLE{personality_emotions_link,
AUTHOR={Hiebler-Ragger, Michaela and Fuchshuber, Jürgen and Dröscher, Heidrun and Vajda, Christian and Fink, Andreas and Unterrainer, Human F.}, 
TITLE={Personality Influences the Relationship Between Primary Emotions and Religious/Spiritual Well-Being},      
JOURNAL={Frontiers in Psychology},      
VOLUME={9},           
YEAR={2018},      
URL={https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00370},       
DOI={10.3389/fpsyg.2018.00370},      
ISSN={1664-1078},   
ABSTRACT={The study of human emotions and personality provides valuable insights into the parameters of mental health and well-being. Affective neuroscience proposes that several levels of emotions – ranging from primary ones such as LUST or FEAR up to higher emotions such as spirituality – interact on a neural level. The present study aimed to further explore this theory. Furthermore, we hypothesized that personality – formed by bottom-up primary emotions and cortical top-down regulation – might act as a link between primary emotions and religious/spiritual well-being. A total sample of 167 (78\% female) student participants completed the Affective Neuroscience Personality Scale (primary emotions), the Big Five Personality Inventory and the Multidimensional Inventory of Religious/Spiritual Well-Being (higher emotions). Correlation analyses confirmed the link between primary and higher emotions as well as their relation to personality. Further regression analyses indicated that personality dimensions mediate the relationship between primary and higher emotions. A substantial interaction between primary emotions, personality dimensions, and religious/spiritual well-being could be confirmed. From a developmental perspective, cortical top-down regulation might influence religious/spiritual well-being by forming relevant personality dimensions. Hence, CARE as well as Agreeableness seem of special importance. Future studies might focus on implications for clinical groups.}
}

@ARTICLE{cross_cultural,
  author={Imran, Ali Shariq and Daudpota, Sher Muhammad and Kastrati, Zenun and Batra, Rakhi},
  journal={IEEE Access}, 
  title={Cross-Cultural Polarity and Emotion Detection Using Sentiment Analysis and Deep Learning on COVID-19 Related Tweets}, 
  year={2020},
  volume={8},
  number={},
  pages={181074-181090},
  doi={10.1109/ACCESS.2020.3027350}}
  
@Book{panksepp_book,
  author    = "J. Panksepp",
  title     = "Affective Neuroscience: The Foundations of Human and Animal Emotions",
  publisher = "NY: Oxford University Press",
  year      = "1998"
}

@incollection{plutchik_model,
title = {Chapter 1 - A GENERAL PSYCHOEVOLUTIONARY THEORY OF EMOTION},
editor = {Robert Plutchik and Henry Kellerman},
booktitle = {Theories of Emotion},
publisher = {Academic Press},
pages = {3-33},
year = {1980},
isbn = {978-0-12-558701-3},
doi = {https://doi.org/10.1016/B978-0-12-558701-3.50007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780125587013500077},
author = {ROBERT PLUTCHIK},
abstract = {ABSTRACT
The general psychoevolutionary theory of emotion that is presented here has a number of important characteristics. First, it provides a broad evolutionary foundation for conceptualizing the domain of emotion as seen in animals and humans. Second, it provides a structural model which describes the interrelations among emotions. Third, it has demonstrated both theoretical and empirical relations among a number of derivative domains including personality traits, diagnoses, and ego defenses. Fourth, it has provided a theoretical rationale for the construction of tests and scales for the measurement of key dimensions within these various domains. Fifth, it has stimulated a good deal of empirical research using these tools and concepts. Finally, the theory provides useful insights into the relationships among emotions, adaptations, and evolution.}
}

@article{fiske1949consistency,
  title={Consistency of the factorial structures of personality ratings from different sources.},
  author={Fiske, Donald W},
  journal={The Journal of Abnormal and Social Psychology},
  volume={44},
  number={3},
  pages={329},
  year={1949},
  publisher={American Psychological Association}
}

@article{smith1967usefulness,
  title={Usefulness of peer ratings of personality in educational research},
  author={Smith, Gene M},
  journal={Educational and Psychological measurement},
  volume={27},
  number={4},
  pages={967--984},
  year={1967},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{norman19672800,
  title={2800 PERSONALITY TRAIT DESCRIPTORS--NORMATIVE OPERATING CHARACTERISTICS FOR A UNIVERSITY POPULATION.},
  author={Norman, Warren T},
  year={1967},
  publisher={ERIC}
}

@article{goldberg1981language,
  title={Language and individual differences: The search for universals in personality lexicons},
  author={Goldberg, Lewis R},
  journal={Review of personality and social psychology},
  volume={2},
  number={1},
  pages={141--165},
  year={1981}
}

@article{mccrae1987validation,
  title={Validation of the five-factor model of personality across instruments and observers.},
  author={McCrae, Robert R and Costa, Paul T},
  journal={Journal of personality and social psychology},
  volume={52},
  number={1},
  pages={81},
  year={1987},
  publisher={American Psychological Association}
}

@ARTICLE{Zhao2018,
  author={Zhao, Guozhen and Ge, Yan and Shen, Biying and Wei, Xingjie and Wang, Hao},
  journal={IEEE Transactions on Affective Computing}, 
  title={Emotion Analysis for Personality Inference from EEG Signals}, 
  year={2018},
  volume={9},
  number={3},
  pages={362-371},
  doi={10.1109/TAFFC.2017.2786207}}

%%%%%%%%%
%Related work
@Article{Ekman1993,
  author  = "Paul Ekman",
  title   = "Facial expression and emotion",
  journal = "American Psychologist",
  year    = "1993",
  volume  = "48",
  number  = "4",
  pages   = "384--392",
  month   = ""
}

@inproceedings{Sebe2005,
author = {Nicu Sebe and Ira Cohen and Theo Gevers and Thomas S. Huang},
title = {{Multimodal approaches for emotion recognition: a survey}},
volume = {5670},
booktitle = {Internet Imaging VI},
editor = {Simone Santini and Raimondo Schettini and Theo Gevers},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {56 -- 67},
keywords = {emotion recognition, multimodal approach, human-computer interaction},
year = {2005},
doi = {10.1117/12.600746},
URL = {https://doi.org/10.1117/12.600746}
}

@misc{Wang2018,
  doi = {10.48550/ARXIV.1811.09362},
  url = {https://arxiv.org/abs/1811.09362},
  author = {Wang, Yansen and Shen, Ying and Liu, Zhun and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{Kumar2020,
  author={Kumar, Ayush and Vepa, Jithendra},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Gated Mechanism for Attention Based Multi Modal Sentiment Analysis}, 
  year={2020},
  volume={},
  number={},
  pages={4477-4481},
  doi={10.1109/ICASSP40776.2020.9053012}}
  
@inproceedings{Poria2017,
    title = "Context-Dependent Sentiment Analysis in User-Generated Videos",
    author = "Poria, Soujanya  and
      Cambria, Erik  and
      Hazarika, Devamanyu  and
      Majumder, Navonil  and
      Zadeh, Amir  and
      Morency, Louis-Philippe",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1081",
    doi = "10.18653/v1/P17-1081",
    pages = "873--883",
    abstract = "Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10{\%} performance improvement over the state of the art and high robustness to generalizability.",
}

@misc{Liu2022,
  doi = {10.48550/ARXIV.2209.02604},
  
  url = {https://arxiv.org/abs/2209.02604},
  
  author = {Liu, Yihe and Yuan, Ziqi and Mao, Huisheng and Liang, Zhiyun and Yang, Wanqiuyue and Qiu, Yuanzhe and Cheng, Tie and Li, Xiaoteng and Xu, Hua and Gao, Kai},
  
  keywords = {Multimedia (cs.MM), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{Han2021,
  doi = {10.48550/ARXIV.2109.00412},
  
  url = {https://arxiv.org/abs/2109.00412},
  
  author = {Han, Wei and Chen, Hui and Poria, Soujanya},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@article{Zhongkai2019,
  author    = {Zhongkai Sun and
               Prathusha Kameswara Sarma and
               William A. Sethares and
               Erik P. Bucy},
  title     = {Multi-modal Sentiment Analysis using Deep Canonical Correlation Analysis},
  journal   = {CoRR},
  volume    = {abs/1907.08696},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.08696},
  eprinttype = {arXiv},
  eprint    = {1907.08696},
  timestamp = {Fri, 20 Mar 2020 09:13:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-08696.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Zhang2022,
author="Zhang, Tonghui
and Dong, Changfei
and Su, Jinsong
and Zhang, Haiying
and Li, Yuzheng",
editor="Lu, Wei
and Huang, Shujian
and Hong, Yu
and Zhou, Xiabing",
title="Unimodal and Multimodal Integrated Representation Learning via Improved Information Bottleneck for Multimodal Sentiment Analysis",
booktitle="Natural Language Processing and Chinese Computing",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="564--576",
abstract="Representation learning is a significant and challenging task in multimodal sentiment analysis (MSA). It aims to improve the performance of model by learning effective unimodal or multimodal representation. To obtain desired characteristics of representation, various constraints are proposed in previous works. However, these constraints are less concerned with the filtering of task-irrelevant information, which is highly correlated with robustness of representation. In this paper, we design a framework based on information bottleneck to filter noise information. By maximizing mutual information between pairwise unimodal representations and minimizing mutual information between unimodal representation and corresponding input, we can promote unimodal representation for including more task-relevant information and filtering out task-irrelevant information. Furthermore, attention bottleneck is embedded into the unimodal encoding process to realize the interaction between different modalities. Then, to improve the discrimination of multimodal representation, we introduce supervised contrastive learning as a constraint of multimodal representation. Last, we conduct extensive experiments on two public multimodal baseline datasets. The experimental results validate the effectiveness of our model.",
isbn="978-3-031-17120-8"
}

@inproceedings{Georgiou2019,
  title={Deep Hierarchical Fusion with Application in Sentiment Analysis},
  author={Efthymios Georgiou and Charilaos Papaioannou and Alexandros Potamianos},
  booktitle={INTERSPEECH},
  year={2019}
}
  
@inproceedings{joshi-etal-2022-cogmen,
    title = "{COGMEN}: {CO}ntextualized {GNN} based Multimodal Emotion recognitio{N}",
    author = "Joshi, Abhinav  and
      Bhat, Ashwani  and
      Jain, Ayush  and
      Singh, Atin  and
      Modi, Ashutosh",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.306",
    doi = "10.18653/v1/2022.naacl-main.306",
    pages = "4148--4164"
}

@misc{mosei_website,
  author       = "MultiComp lab",
  title        = "CMU-MOSEI Dataset",
  howpublished = "\url{http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/}",
  note         = "[online] accessed 17.11.2022"
}

%%%%%%%
% Datasets
@article{mosi_dataset,
  doi = {10.48550/ARXIV.1606.06259},
  url = {https://arxiv.org/abs/1606.06259},
  author = {Zadeh, Amir and Zellers, Rowan and Pincus, Eli and Morency, Louis-Philippe},
  keywords = {Computation and Language (cs.CL), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{mosei_dataset,
    title = "Multimodal Language Analysis in the Wild: {CMU}-{MOSEI} Dataset and Interpretable Dynamic Fusion Graph",
    author = "Bagher Zadeh, AmirAli  and
      Liang, Paul Pu  and
      Poria, Soujanya  and
      Cambria, Erik  and
      Morency, Louis-Philippe",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1208",
    doi = "10.18653/v1/P18-1208",
    pages = "2236--2246"
}

@inproceedings{youtube_dataset,
author = {Morency, Louis-Philippe and Mihalcea, Rada and Doshi, Payal},
title = {Towards Multimodal Sentiment Analysis: Harvesting Opinions from the Web},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070509},
doi = {10.1145/2070481.2070509},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {169–176},
numpages = {8},
keywords = {YouTube videos, audio-visual integration, multimodal signal processing, subjectivity and sentiment analysis},
location = {Alicante, Spain},
series = {ICMI '11}
}

@misc{meld_dataset,
  doi = {10.48550/ARXIV.1810.02508},
  url = {https://arxiv.org/abs/1810.02508},
  author = {Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations},
  publisher = {arXiv},
  year = {2018},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@inproceedings{ch-sims_dataset,
    title = "{CH}-{SIMS}: A {C}hinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality",
    author = "Yu, Wenmeng  and
      Xu, Hua  and
      Meng, Fanyang  and
      Zhu, Yilin  and
      Ma, Yixiao  and
      Wu, Jiele  and
      Zou, Jiyun  and
      Yang, Kaicheng",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.343",
    doi = "10.18653/v1/2020.acl-main.343",
    pages = "3718--3727",
}

@Article{iemocap_dataset,
  author  = "Carlos Busso and Murtaza Bulut and Chi-Chun Lee and Abe Kazemzadeh and Emily Mower and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan",
  title   = "IEMOCAP: interactive emotional dyadic motion capture database",
  journal = "Language Resources and Evaluation",
  year    = "2008",
  volume  = "42",
  pages   = "335--359",
  month   = "November"
}

@ARTICLE{sal_dataset,
  author={McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schroder, Marc},
  journal={IEEE Transactions on Affective Computing}, 
  title={The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent}, 
  year={2012},
  volume={3},
  number={1},
  pages={5-17},
  doi={10.1109/T-AFFC.2011.20}}
  
@ARTICLE{musecar_dataset,
  author={Stappen, Lukas and Baird, Alice and Schumann, Lea and Bjorn, Schuller},
  journal={IEEE Transactions on Affective Computing}, 
  title={The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TAFFC.2021.3097002}}
  
@article{memotion_dataset,
  title={Memotion 2: Dataset on Sentiment and Emotion Analysis of Memes},
  author={Ramamoorthy, Sathyanarayanan and Gunti, Nethra and Mishra, Shreyash and Suryavardan, S and Reganti, Aishwarya and Patwa, Parth and Das, Amitava and Chakraborty, Tanmoy and Sheth, Amit and Ekbal, Asif and others},
  year={2021}
}

%%%%%%%%%%%%%%%%
% Answering rq
@article{rosas2013multimodal,
  title={Multimodal sentiment analysis of spanish online videos},
  author={Rosas, Ver{\'o}nica P{\'e}rez and Mihalcea, Rada and Morency, Louis-Philippe},
  journal={IEEE Intelligent Systems},
  volume={28},
  number={3},
  pages={38--45},
  year={2013},
  publisher={IEEE}
}

@article{poria2015towards,
  title={Towards an intelligent framework for multimodal affective data analysis},
  author={Poria, Soujanya and Cambria, Erik and Hussain, Amir and Huang, Guang-Bin},
  journal={Neural Networks},
  volume={63},
  pages={104--116},
  year={2015},
  publisher={Elsevier}
}

@article{park2016multimodal,
  title={Multimodal analysis and prediction of persuasiveness in online social multimedia},
  author={Park, Sunghyun and Shim, Han Suk and Chatterjee, Moitreya and Sagae, Kenji and Morency, Louis-Philippe},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={6},
  number={3},
  pages={1--25},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@incollection{cai2015convolutional,
  title={Convolutional neural networks for multimedia sentiment analysis},
  author={Cai, Guoyong and Xia, Binbin},
  booktitle={Natural language processing and Chinese computing},
  pages={159--167},
  year={2015},
  publisher={Springer}
}

@inproceedings{poria2015deep,
  title={Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis},
  author={Poria, Soujanya and Cambria, Erik and Gelbukh, Alexander},
  booktitle={Proceedings of the 2015 conference on empirical methods in natural language processing},
  pages={2539--2544},
  year={2015}
}

@inproceedings{zadeh2018multi,
  title={Multi-attention recurrent network for human communication comprehension},
  author={Zadeh, Amir and Liang, Paul Pu and Poria, Soujanya and Vij, Prateek and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{mai2019divide,
  title={Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing},
  author={Mai, Sijie and Hu, Haifeng and Xing, Songlong},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={481--492},
  year={2019}
}

@inproceedings{zadeh2018memory,
  title={Memory fusion network for multi-view sequential learning},
  author={Zadeh, Amir and Liang, Paul Pu and Mazumder, Navonil and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}

@article{msa_review,
title = {Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions},
journal = {Information Fusion},
volume = {91},
pages = {424-444},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
author = {Ankita Gandhi and Kinjal Adhvaryu and Soujanya Poria and Erik Cambria and Amir Hussain},
keywords = {Affective computing, Sentiment analysis, Multimodal fusion, Fusion techniques},
abstract = {Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for multimodal feature extraction and fusion and sentiment polarity detection, with aims to minimize error rate and improve performance. This survey paper examines primary taxonomy and newly released multimodal fusion architectures. Recent developments in MSA architectures are divided into ten categories, namely early fusion, late fusion, hybrid fusion, model-level fusion, tensor fusion, hierarchical fusion, bi-modal fusion, attention-based fusion, quantum-based fusion and word-level fusion. A comparison of several architectural evolutions in terms of MSA fusion categories and their relative strengths and limitations are presented. Finally, a number of interdisciplinary applications and future research directions are proposed.}
}

@inproceedings{cm-bert-sota,
author = {Yang, Kaicheng and Xu, Hua and Gao, Kai},
title = {CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413690},
doi = {10.1145/3394171.3413690},
abstract = {Multimodal sentiment analysis is an emerging research field that aims to enable machines to recognize, interpret, and express emotion. Through the cross-modal interaction, we can get more comprehensive emotional characteristics of the speaker. Bidirectional Encoder Representations from Transformers (BERT) is an efficient pre-trained language representation model. Fine-tuning it has obtained new state-of-the-art results on eleven natural language processing tasks like question answering and natural language inference. However, most previous works fine-tune BERT only base on text data, how to learn a better representation by introducing the multimodal information is still worth exploring. In this paper, we propose the Cross-Modal BERT (CM-BERT), which relies on the interaction of text and audio modality to fine-tune the pre-trained BERT model. As the core unit of the CM-BERT, masked multimodal attention is designed to dynamically adjust the weight of words by combining the information of text and audio modality. We evaluate our method on the public multimodal sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experiment results show that it has significantly improved the performance on all the metrics over previous baselines and text-only finetuning of BERT. Besides, we visualize the masked multimodal attention and proves that it can reasonably adjust the weight of words by introducing audio modality information.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {521–528},
numpages = {8},
keywords = {attention network, multimodal sentiment analysis, pretrained model},
location = {Seattle, WA, USA},
series = {MM '20}
}

%%%%%%%
% Choice of methods
@INPROCEEDINGS{openface,  author={Baltrusaitis, Tadas and Zadeh, Amir and Lim, Yao Chong and Morency, Louis-Philippe},  booktitle={2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)},   title={OpenFace 2.0: Facial Behavior Analysis Toolkit},   year={2018},  volume={},  number={},  pages={59-66},  doi={10.1109/FG.2018.00019}}

@inproceedings{opensmile,
author = {Eyben, Florian and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn},
title = {Opensmile: The Munich Versatile and Fast Open-Source Audio Feature Extractor},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874246},
doi = {10.1145/1873951.1874246},
abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1459–1462},
numpages = {4},
keywords = {statistical functionals, audio feature extraction, signal processing, speech, emotion, music},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{sBERT,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

%%%%%%%
% Risk analysis
@phdthesis{risk_method,
  author  = "Walid Demloj and Kjetil Grosberghaugen and Julian Nyland Skattum and Hans Petter Fauchald Taralrud",
  title   = "Face Image Quality Assessment",
  school  = "Norwegian University of Science and Technology",
  year    = "2021",
  type    = "Bachelor thesis",
  month   = "June"
}